{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFEPIEzkYMALdnmdS5fkzY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjanb/Machine-Learning-basics/blob/main/sk_learn_for_traditional_machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAAKSmH5vV5N",
        "outputId": "2972a9ba-f80d-412d-9eb2-546bbf92cdf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframes:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0          -0.900681          1.019004          -1.340227         -1.315444   \n",
            "1          -1.143017         -0.131979          -1.340227         -1.315444   \n",
            "2          -1.385353          0.328414          -1.397064         -1.315444   \n",
            "3          -1.506521          0.098217          -1.283389         -1.315444   \n",
            "4          -1.021849          1.249201          -1.340227         -1.315444   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Scaled dataframes:\n",
            "Shape of x_train: (120, 4)\n",
            "Shape of x_test: (30, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load iris datasets\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "print(\"Original dataframes:\")\n",
        "print(df.head())\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "df.iloc[:, :-1] = scaler.fit_transform(df.iloc[:, :-1])\n",
        "print(df.head())\n",
        "print(\"\\nScaled dataframes:\")\n",
        "\n",
        "# Train test split correction\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f\"Shape of x_train: {x_train.shape}\")\n",
        "print(f\"Shape of x_test: {x_test.shape}\")"
      ]
    }
  ]
}